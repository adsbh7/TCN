{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TCN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BGmelVYKm8S",
        "outputId": "3bce9034-985e-4a81-e69d-473bcc9165a1"
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0GTlpsoJLrg"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras import backend as K, Model, Input, optimizers\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Activation, SpatialDropout1D, Lambda\n",
        "from tensorflow.keras.layers import Layer, Conv1D, MaxPooling1D, Dense, BatchNormalization, LayerNormalization\n",
        "import inspect\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "class5 = {0:'Exploit Kit', 1:'malspam', 2:'Phishing', 3:'Ransomware', 4:'Trojan'}\n",
        "learning_rate = 0.002  \n",
        "filters_num = 16\n",
        "filter_size = 8\n",
        "TRAIN_EPOCHS = 100\n",
        "dilation_num = 9\n",
        "\n",
        "def update_confusion_matrix(confusion_matrix, actual_lb, predict_lb):\n",
        "    for idx, value in enumerate(actual_lb):\n",
        "        p_value = predict_lb[idx]\n",
        "        confusion_matrix[value, p_value] += 1\n",
        "    return confusion_matrix\n",
        "\n",
        "def truncate(f, n):\n",
        "    trunc_f = np.math.floor(f * 10 ** n) / 10 ** n\n",
        "    return '{:.4f}'.format(trunc_f) # only for 0.0 => 0.0000\n",
        "\n",
        "def get_data():\n",
        "    print('[*] load data')\n",
        "    #os.getcwd()         # check current directory\n",
        "    with tf.device('/GPU:0'):\n",
        "      data = np.loadtxt('/content/gdrive/My Drive/tcn/train_final.csv', delimiter=\",\", dtype=np.float32)\n",
        "      test = np.loadtxt('/content/gdrive/My Drive/tcn/test_final.csv', delimiter=\",\", dtype=np.float32)\n",
        "      print('[-] done!')\n",
        "      tr_x = data[:,:-1]\n",
        "      te_x = test[:,:-1]\n",
        "      #print(tr_x.shape)\n",
        "      tr_x = np.expand_dims(tr_x, axis=2)\n",
        "      te_x = np.expand_dims(te_x, axis=2)\n",
        "    \n",
        "      tr_y = data[:,-1]\n",
        "      tr_y = to_categorical(tr_y, 5)   # one-hot encoding\n",
        "      tr_y = np.expand_dims(tr_y, axis=2)\n",
        "      te_y = test[:,-1]\n",
        "      te_y = to_categorical(te_y, 5)   # one-hot encoding\n",
        "      te_y = np.expand_dims(te_y, axis=2)\n",
        "    #print(len(data))   # 2400\n",
        "  \n",
        "    return tr_x, tr_y, te_x, te_y\n",
        "\n",
        "\n",
        "def is_power_of_two(num):     # return : True or False\n",
        "    return num != 0 and ((num & (num - 1)) == 0)\n",
        "\n",
        "\n",
        "def adjust_dilations(dilations):   # if dilataions == True return dilations \n",
        "    if all([is_power_of_two(i) for i in dilations]): \n",
        "        return dilations\n",
        "    else:\n",
        "        new_dilations = [2 ** i for i in dilations]\n",
        "        return new_dilations\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aELzW_jyJLr2"
      },
      "source": [
        "class ResidualBlock(Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 dilation_rate,                            # dilation power of 2\n",
        "                 nb_filters,                               # number of conv filters\n",
        "                 kernel_size,                              # conv kernel size\n",
        "                 padding,                                  # valid(no padding), same(inputlen=outputlen), causal\n",
        "                 activation='relu',                        # o = Activation(x + F(x))\n",
        "                 dropout_rate=0,                           # [0, 1]\n",
        "                 kernel_initializer='he_normal',      \n",
        "                 use_batch_norm=True,                     \n",
        "                 use_layer_norm=False,\n",
        "                 last_block=True,\n",
        "                 **kwargs):                                # key, value \n",
        "\n",
        "\n",
        "        self.dilation_rate = dilation_rate\n",
        "        self.nb_filters = nb_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "        self.activation = activation\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.use_layer_norm = use_layer_norm\n",
        "        self.last_block = last_block\n",
        "        \n",
        "        self.layers = []\n",
        "        self.layers_outputs = []\n",
        "        self.shape_match_conv = None\n",
        "        self.res_output_shape = None\n",
        "        self.final_activation = None\n",
        "\n",
        "        super(ResidualBlock, self).__init__(**kwargs)\n",
        "\n",
        "    def _add_and_activate_layer(self, layer):               # append layer to internal layer list\n",
        "        self.layers.append(layer)\n",
        "        self.layers[-1].build(self.res_output_shape)\n",
        "        self.res_output_shape = self.layers[-1].compute_output_shape(self.res_output_shape)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        with K.name_scope(self.name):  # name scope used to make sure weights get unique names\n",
        "            self.layers = []\n",
        "            self.res_output_shape = input_shape\n",
        "\n",
        "            for k in range(2):\n",
        "                with K.name_scope('conv1D_{}'.format(k)):         # conv1D_0 or conv1D_1\n",
        "                    self._add_and_activate_layer(Conv1D(filters=self.nb_filters,\n",
        "                                                        kernel_size=self.kernel_size,\n",
        "                                                        dilation_rate=self.dilation_rate,\n",
        "                                                        padding=self.padding,\n",
        "                                                        name='conv1D_{}'.format(k),\n",
        "                                                        kernel_initializer=self.kernel_initializer))\n",
        "                    #self._add_and_activate_layer(MaxPooling1D(pool_size=3))\n",
        "\n",
        "                with K.name_scope('norm_{}'.format(k)):           # norm_0 or norm_1\n",
        "                    if self.use_batch_norm:\n",
        "                        self._add_and_activate_layer(BatchNormalization())\n",
        "                        print('use batch_norm')\n",
        "                    elif self.use_layer_norm:\n",
        "                        self._add_and_activate_layer(LayerNormalization())\n",
        "                        print('use layer_norm')\n",
        "\n",
        "                self._add_and_activate_layer(Activation('relu'))\n",
        "                self._add_and_activate_layer(SpatialDropout1D(rate=self.dropout_rate))\n",
        "\n",
        "            if not self.last_block:\n",
        "                # 1x1 conv to match the shapes (channel dimension).\n",
        "                name = 'conv1D_{}'.format(k + 1)\n",
        "                with K.name_scope(name):\n",
        "                    # make and build this layer separately because it directly uses input_shape\n",
        "                    self.shape_match_conv = Conv1D(filters=self.nb_filters,\n",
        "                                                   kernel_size=1,\n",
        "                                                   padding='same',            # len(input) == len(output)\n",
        "                                                   name=name,\n",
        "                                                   kernel_initializer=self.kernel_initializer)\n",
        "\n",
        "            else:\n",
        "                self.shape_match_conv = Lambda(lambda x: x, name='identity')   # Lambda : Layer로 감싸줌\n",
        "\n",
        "            self.shape_match_conv.build(input_shape)\n",
        "            self.res_output_shape = self.shape_match_conv.compute_output_shape(input_shape)\n",
        "\n",
        "            self.final_activation = Activation(self.activation)\n",
        "            self.final_activation.build(self.res_output_shape)  # probably isn't necessary\n",
        "\n",
        "            # this is done to force Keras to add the layers in the list to self._layers\n",
        "            for layer in self.layers:\n",
        "                self.__setattr__(layer.name, layer)\n",
        "\n",
        "            super(ResidualBlock, self).build(input_shape)  # done to make sure self.built is set True\n",
        "\n",
        "    def call(self, inputs, training=None):                 # training : boolean whether it's training mode or not\n",
        "        x = inputs                           \n",
        "        self.layers_outputs = [x]\n",
        "        for layer in self.layers:\n",
        "            training_flag = 'training' in dict(inspect.signature(layer.call).parameters)\n",
        "            x = layer(x, training=training) if training_flag else layer(x)\n",
        "            # training_flag == False -> x = layer(x)\n",
        "            self.layers_outputs.append(x)\n",
        "        x2 = self.shape_match_conv(inputs)\n",
        "        self.layers_outputs.append(x2)\n",
        "        res_x = layers.add([x2, x])\n",
        "        self.layers_outputs.append(res_x)\n",
        "\n",
        "        res_act_x = self.final_activation(res_x)            \n",
        "        self.layers_outputs.append(res_act_x)\n",
        "        return [res_act_x, x]                               # residual model tensor, skip connection tensor\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return [self.res_output_shape, self.res_output_shape]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWWdHJjqJLsD"
      },
      "source": [
        "class TCN(Layer):              # input shape (batch_size, timesteps, input_dim), return TCN layer\n",
        "    def __init__(self,\n",
        "                 nb_filters=64,\n",
        "                 kernel_size=16,\n",
        "                 nb_stacks=1,\n",
        "                 dilations=(1, 2, 4, 8, 16, 32),\n",
        "                 padding='causal',\n",
        "                 use_skip_connections=True,\n",
        "                 dropout_rate=0.0,\n",
        "                 return_sequences=False,\n",
        "                 activation='linear',\n",
        "                 kernel_initializer='he_normal',\n",
        "                 use_batch_norm=False,\n",
        "                 use_layer_norm=False,\n",
        "                 **kwargs):                        # to configure parent class layer\n",
        "        #print('__init__\\n')\n",
        "        self.return_sequences = return_sequences\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.use_skip_connections = use_skip_connections\n",
        "        self.dilations = dilations\n",
        "        self.nb_stacks = nb_stacks\n",
        "        self.kernel_size = kernel_size\n",
        "        self.nb_filters = nb_filters\n",
        "        self.activation = activation\n",
        "        self.padding = padding\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.use_layer_norm = use_layer_norm\n",
        "        self.skip_connections = []\n",
        "        self.residual_blocks = []\n",
        "        self.layers_outputs = []\n",
        "        self.main_conv1D = None\n",
        "        self.build_output_shape = None\n",
        "        self.lambda_layer = None\n",
        "        self.lambda_ouput_shape = None\n",
        "\n",
        "        if padding != 'causal' and padding != 'same':   # Must use padding\n",
        "            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n",
        "\n",
        "        if not isinstance(nb_filters, int):          # check whether nb_filters(64) is integer or not\n",
        "            print('An interface change occurred after the version 2.1.2.')\n",
        "            print('Before: tcn.TCN(x, return_sequences=False, ...)')\n",
        "            print('Now should be: tcn.TCN(return_sequences=False, ...)(x)')\n",
        "            print('The alternative is to downgrade to 2.1.2 (pip install keras-tcn==2.1.2).')\n",
        "            raise Exception()\n",
        "\n",
        "        # initialize parent class\n",
        "        super(TCN, self).__init__(**kwargs)\n",
        "\n",
        "    @property     # getter\n",
        "    def receptive_field(self):\n",
        "        assert_msg = 'The receptive field formula works only with power of two dilations.'\n",
        "        assert all([is_power_of_two(i) for i in self.dilations]), assert_msg\n",
        "        # check dilation numbers are power of two or not / if not true, raise assert error\n",
        "        return self.kernel_size * self.nb_stacks * self.dilations[-1]\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        #print('build\\n')\n",
        "        \n",
        "        self.main_conv1D = Conv1D(filters=64,#self.nb_filters,    # of filters\n",
        "                                  kernel_size=10,              # same with Dense layer\n",
        "                                  padding=self.padding,       # causal\n",
        "                                  kernel_initializer=self.kernel_initializer)  # he_normal\n",
        "        '''\n",
        "        block = Conv1D(filters=64,#self.nb_filters,    # of filters\n",
        "                       kernel_size=10,              # same with Dense layer\n",
        "                       padding=self.padding,       # causal\n",
        "                       kernel_initializer=self.kernel_initializer)  # he_normal\n",
        "        block = MaxPooling1D(pool_size=3)(block)\n",
        "        self.main_conv1D = block\n",
        "        '''\n",
        "        self.main_conv1D.build(input_shape)\n",
        "\n",
        "        # member to hold current output shape of the layer for building purposes\n",
        "        self.build_output_shape = self.main_conv1D.compute_output_shape(input_shape)\n",
        "\n",
        "        # list to hold all the member ResidualBlocks\n",
        "        self.residual_blocks = []\n",
        "        total_num_blocks = self.nb_stacks * len(self.dilations)     # block = layer, total_num_blocks = 6\n",
        "        if not self.use_skip_connections:\n",
        "            total_num_blocks += 1  # cheap way to do a false case for below\n",
        "\n",
        "        for s in range(self.nb_stacks):\n",
        "            for d in self.dilations:\n",
        "                self.residual_blocks.append(ResidualBlock(dilation_rate=d,\n",
        "                                                          nb_filters=self.nb_filters,\n",
        "                                                          kernel_size=self.kernel_size,\n",
        "                                                          padding=self.padding,\n",
        "                                                          activation=self.activation,\n",
        "                                                          dropout_rate=self.dropout_rate,\n",
        "                                                          use_batch_norm=self.use_batch_norm,\n",
        "                                                          use_layer_norm=self.use_layer_norm,\n",
        "                                                          kernel_initializer=self.kernel_initializer,\n",
        "                                                          last_block=len(self.residual_blocks) + 1 == total_num_blocks,\n",
        "                                                          name='residual_block_{}'.format(len(self.residual_blocks))))\n",
        "                # build newest residual block\n",
        "                self.residual_blocks[-1].build(self.build_output_shape)\n",
        "                self.build_output_shape = self.residual_blocks[-1].res_output_shape\n",
        "\n",
        "        # this is done to force keras to add the layers in the list to self._layers\n",
        "        for layer in self.residual_blocks:\n",
        "            self.__setattr__(layer.name, layer)\n",
        "\n",
        "        # Author: @karolbadowski.\n",
        "        output_slice_index = int(self.build_output_shape.as_list()[1] / 2) if self.padding == 'same' else -1\n",
        "        #print(self.build_output_shape.as_list())   #(None, 10000, nb_filters)\n",
        "        #print('output slice index : ', output_slice_index)   # -1\n",
        "        self.lambda_layer = Lambda(lambda tt: tt[:, output_slice_index, :])\n",
        "        self.lambda_ouput_shape = self.lambda_layer.compute_output_shape(self.build_output_shape)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if not self.built:\n",
        "            self.build(input_shape)\n",
        "        if not self.return_sequences:\n",
        "            return self.lambda_ouput_shape\n",
        "        else:\n",
        "            return self.build_output_shape\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        #print('call\\n')\n",
        "        x = inputs\n",
        "        self.layers_outputs = [x]\n",
        "        try:\n",
        "            x = self.main_conv1D(x)\n",
        "            self.layers_outputs.append(x)\n",
        "        except AttributeError:\n",
        "            print('The backend of keras-tcn>2.8.3 has changed from keras to tensorflow.keras.')\n",
        "            print('Either update your imports:\\n- From \"from keras.layers import <LayerName>\" '\n",
        "                  '\\n- To \"from tensorflow.keras.layers import <LayerName>\"')\n",
        "            print('Or downgrade to 2.8.3 by running \"pip install keras-tcn==2.8.3\"')\n",
        "            import sys\n",
        "            sys.exit(0)\n",
        "        self.skip_connections = []\n",
        "        for layer in self.residual_blocks:\n",
        "            x, skip_out = layer(x, training=training)\n",
        "            self.skip_connections.append(skip_out)\n",
        "            self.layers_outputs.append(x)\n",
        "\n",
        "        if self.use_skip_connections:\n",
        "            x = layers.add(self.skip_connections)\n",
        "            self.layers_outputs.append(x)\n",
        "        if not self.return_sequences:\n",
        "            x = self.lambda_layer(x)\n",
        "            self.layers_outputs.append(x)\n",
        "        return x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TCN, self).get_config()\n",
        "        config['nb_filters'] = self.nb_filters\n",
        "        config['kernel_size'] = self.kernel_size\n",
        "        config['nb_stacks'] = self.nb_stacks\n",
        "        config['dilations'] = self.dilations\n",
        "        config['padding'] = self.padding\n",
        "        config['use_skip_connections'] = self.use_skip_connections\n",
        "        config['dropout_rate'] = self.dropout_rate\n",
        "        config['return_sequences'] = self.return_sequences\n",
        "        config['activation'] = self.activation\n",
        "        config['use_batch_norm'] = self.use_batch_norm\n",
        "        config['use_layer_norm'] = self.use_layer_norm\n",
        "        config['kernel_initializer'] = self.kernel_initializer\n",
        "        return config"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNuKUzfXJLsS"
      },
      "source": [
        "def compiled_tcn(num_feat,  # type: int\n",
        "                 num_classes,  # type: int                size of final dense layer(# of classes)\n",
        "                 nb_filters,  # type: int                 \n",
        "                 kernel_size,  # type: int\n",
        "                 dilations,  # type: List[int]\n",
        "                 nb_stacks,  # type: int\n",
        "                 max_len,  # type: int                    max sequence length\n",
        "                 output_len=1,  # type: int\n",
        "                 padding='causal',  # type: str\n",
        "                 use_skip_connections=True,  # type: bool\n",
        "                 return_sequences=True,\n",
        "                 regression=False,  # type: bool          continuous or discrete\n",
        "                 dropout_rate=0.05,  # type: float\n",
        "                 name='tcn',  # type: str,\n",
        "                 kernel_initializer='he_normal',  # type: str,\n",
        "                 activation='linear',  # type:str,\n",
        "                 opt='adam',                              # optimizer name\n",
        "                 lr=learning_rate,                                # learning rate\n",
        "                 use_batch_norm=False,\n",
        "                 use_layer_norm=False):\n",
        "    #print('compiled_tcn\\n')\n",
        "    dilations = adjust_dilations(dilations)    # whether dilaitions are 2** or not / return dilations list\n",
        "\n",
        "    input_layer = Input(shape=(max_len, num_feat))\n",
        "    #print('input : ', input_layer)    # input shape : (None, 10000, 1)\n",
        "    \n",
        "    filters1 = 32\n",
        "    filters2 = 64\n",
        "    block = Conv1D(filters=filters1,\n",
        "                   kernel_size=8,\n",
        "                   padding='same',\n",
        "                   activation='relu',\n",
        "                   strides=2)(input_layer)\n",
        "    block = MaxPooling1D(pool_size=3)(block)\n",
        "\n",
        "       \n",
        "    block = Conv1D(filters=filters2,\n",
        "                   kernel_size=8,\n",
        "                   padding='same',\n",
        "                   activation='relu',\n",
        "                   strides=2)(block)\n",
        "    block = MaxPooling1D(pool_size=3)(block)\n",
        "    \n",
        "    '''\n",
        "    block = Conv1D(filters=filters2,\n",
        "                   kernel_size=8,\n",
        "                   padding='same',\n",
        "                   activation='relu',\n",
        "                   strides=2)(block)\n",
        "    block = MaxPooling1D(pool_size=3)(block)\n",
        "    '''\n",
        "    x = TCN(nb_filters, kernel_size, nb_stacks, dilations, padding,\n",
        "            use_skip_connections, dropout_rate, return_sequences,\n",
        "            activation, kernel_initializer, use_batch_norm, use_layer_norm,\n",
        "            name=name)(block)\n",
        "    \n",
        "\n",
        "    #print('x.shape=', x.shape)   # x.shape : (None, nb_filters)\n",
        "\n",
        "    def get_opt():\n",
        "        if opt == 'adam':\n",
        "            return optimizers.Adam(lr=lr, clipnorm=1.)\n",
        "        elif opt == 'rmsprop':\n",
        "            return optimizers.RMSprop(lr=lr, clipnorm=1.)\n",
        "        else:\n",
        "            raise Exception('Only Adam and RMSProp are available here')\n",
        "\n",
        "    if not regression:\n",
        "        print('[-] do classification...')\n",
        "        #x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "        x = Dense(num_classes)(x)\n",
        "        x = Activation('softmax')(x)\n",
        "        output_layer = x\n",
        "        model = Model(input_layer, output_layer)\n",
        "\n",
        "        # https://github.com/keras-team/keras/pull/11373\n",
        "        def accuracy(y_true, y_pred):\n",
        "        # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "            if K.ndim(y_true) == K.ndim(y_pred):\n",
        "                y_true = K.squeeze(y_true, -1)\n",
        "                # convert dense predictions to labels\n",
        "                y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "                y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "                return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
        "\n",
        "        model.compile(get_opt(), loss='sparse_categorical_crossentropy', metrics=[accuracy])\n",
        "\n",
        "    else:\n",
        "        # regression\n",
        "        x = Dense(output_len)(x)\n",
        "        x = Activation('linear')(x)\n",
        "        output_layer = x\n",
        "        model = Model(input_layer, output_layer)\n",
        "        model.compile(get_opt(), loss='mean_squared_error')\n",
        "    print('model.x = {}'.format(input_layer.shape))\n",
        "    print('model.y = {}'.format(output_layer.shape))\n",
        "    return model\n",
        "\n",
        "\n",
        "def tcn_full_summary(model, expand_residual_blocks=True):\n",
        "    layers = model._layers.copy()  # store existing layers\n",
        "    model._layers.clear()  # clear layers\n",
        "\n",
        "    for i in range(len(layers)):\n",
        "        if isinstance(layers[i], TCN):\n",
        "            for layer in layers[i]._layers:\n",
        "                if not isinstance(layer, ResidualBlock):\n",
        "                    if not hasattr(layer, '__iter__'):\n",
        "                        model._layers.append(layer)\n",
        "                else:\n",
        "                    if expand_residual_blocks:\n",
        "                        for lyr in layer._layers:\n",
        "                            if not hasattr(lyr, '__iter__'):\n",
        "                                model._layers.append(lyr)\n",
        "                    else:\n",
        "                        model._layers.append(layer)\n",
        "        else:\n",
        "            model._layers.append(layers[i])\n",
        "\n",
        "    model.summary()  # print summary\n",
        "\n",
        "    # restore original layers\n",
        "    model._layers.clear()\n",
        "    [model._layers.append(lyr) for lyr in layers]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSPmB5ReJLsg",
        "outputId": "d3dc2b18-1da2-4fac-f4e3-9c70dc1c65c7"
      },
      "source": [
        "def do_tcn(tr_x, tr_y, te_x, te_y):\n",
        "    model = compiled_tcn(return_sequences=False,\n",
        "                         num_feat=1,\n",
        "                         num_classes=5,\n",
        "                         nb_filters=filters_num,\n",
        "                         kernel_size=filter_size,\n",
        "                         lr=learning_rate,\n",
        "                         dilations=[2 ** i for i in range(dilation_num)],\n",
        "                         nb_stacks=1,\n",
        "                         max_len=tr_x[0:1].shape[1],\n",
        "                         use_skip_connections=True)\n",
        "    \n",
        "    print(f'x_train.shape = {tr_x.shape}')\n",
        "    print(f'y_train.shape = {tr_y.shape}')\n",
        "    \n",
        "    print(f'x_test.shape = {te_x.shape}')\n",
        "    print(f'y_test.shape = {te_y.shape}')\n",
        "    \n",
        "    tcn_full_summary(model, expand_residual_blocks=False)\n",
        "    \n",
        "    #tr_y.squeeze().argmax(axis=1) shape : (2400,)\n",
        "    \n",
        "    \n",
        "    with tf.device('/GPU:0'):\n",
        "      model.fit(tr_x, tr_y.squeeze().argmax(axis=1), epochs=TRAIN_EPOCHS, batch_size=64,\n",
        "                validation_data=(te_x, te_y.squeeze().argmax(axis=1)))\n",
        "    \n",
        "    model.evaluate(te_x, te_y.squeeze().argmax(axis=1))\n",
        "    predictions = model.predict(te_x)\n",
        "    #print(result.shape)   #(600, 5)\n",
        "\n",
        "    # stat and save\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "    true_labels = np.argmax(te_y, axis=1)\n",
        "    if len(predicted_labels) > len(true_labels):\n",
        "        num_pad = len(predicted_labels) - len(true_labels)\n",
        "        true_labels = np.concatenate([true_labels, true_labels[0:num_pad]])\n",
        "    #print(len(predicted_labels))\n",
        "    #print(len(true_labels))\n",
        "    len_test = len(true_labels)\n",
        "    cf_ma = np.zeros((5,5), dtype=int)\n",
        "    #print(true_labels, predicted_labels)\n",
        "    update_confusion_matrix(cf_ma, true_labels, predicted_labels)\n",
        "    metrics_list = []\n",
        "    for i in range(5):\n",
        "        acc = truncate((float(len_test-cf_ma[:,i].sum()-cf_ma[i,:].sum()+cf_ma[i,i]*2)/len_test)*100, 4)\n",
        "        metrics_list.append([class5[i], str(i), str(cf_ma[i,0]), str(cf_ma[i,1]), str(cf_ma[i,2]), str(cf_ma[i,3]), str(cf_ma[i,4]), str(acc)])\n",
        "    overall_acc = truncate((float(cf_ma[0,0]+cf_ma[1,1]+cf_ma[2,2]+cf_ma[3,3]+cf_ma[4,4])/len_test)*100, 4)\n",
        "\n",
        "    filename = 'test_result.txt'\n",
        "    print('[*] Writing output text file...\\n\\n')\n",
        "    with open(filename,'w') as f:\n",
        "        f.write(\"\\n\")\n",
        "        f.write('CLASS_NUM: 5\\n')\n",
        "        f.write('learning rate: ' + str(learning_rate) + \"\\n\")\n",
        "        f.write('TRAIN_EPOCHS: ' + str(TRAIN_EPOCHS) + \"\\n\")\n",
        "        f.write('filters_num: ' + str(filters_num) + \"\\n\")\n",
        "        f.write('filter_size: ' + str(filter_size) + \"\\n\")\n",
        "        f.write('dilation_num: ' + str(dilation_num) + \"\\n\")\n",
        "        f.write(\"label\\tindex\\t0\\t1\\t2\\t3\\t4\\tACC\\n\")\n",
        "        for metrics in metrics_list:\n",
        "            f.write('\\t'.join(metrics) + \"\\n\")\n",
        "        f.write('Overall accuracy: ' + str(overall_acc) + \"\\n\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    tr_x, tr_y, te_x, te_y = get_data()\n",
        "    #print(tr_x.max(axis=0), te_x.max(axis=0)) # max 15\n",
        "    do_tcn(tr_x, tr_y, te_x, te_y)\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] load data\n",
            "[-] done!\n",
            "[-] do classification...\n",
            "model.x = (None, 10000, 1)\n",
            "model.y = (None, 5)\n",
            "x_train.shape = (2400, 10000, 1)\n",
            "y_train.shape = (2400, 5, 1)\n",
            "x_test.shape = (600, 10000, 1)\n",
            "y_test.shape = (600, 5, 1)\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 10000, 1)]        0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 5000, 32)          288       \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 1666, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 833, 64)           16448     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 277, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 277, 64)           41024     \n",
            "_________________________________________________________________\n",
            "residual_block_0 (ResidualBl [(None, 277, 16), (None,  11312     \n",
            "_________________________________________________________________\n",
            "residual_block_1 (ResidualBl [(None, 277, 16), (None,  4400      \n",
            "_________________________________________________________________\n",
            "residual_block_2 (ResidualBl [(None, 277, 16), (None,  4400      \n",
            "_________________________________________________________________\n",
            "residual_block_3 (ResidualBl [(None, 277, 16), (None,  4400      \n",
            "_________________________________________________________________\n",
            "residual_block_4 (ResidualBl [(None, 277, 16), (None,  4400      \n",
            "_________________________________________________________________\n",
            "residual_block_5 (ResidualBl [(None, 277, 16), (None,  4400      \n",
            "_________________________________________________________________\n",
            "residual_block_6 (ResidualBl [(None, 277, 16), (None,  4400      \n",
            "_________________________________________________________________\n",
            "residual_block_7 (ResidualBl [(None, 277, 16), (None,  4400      \n",
            "_________________________________________________________________\n",
            "residual_block_8 (ResidualBl [(None, 277, 16), (None,  4128      \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 85        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 104,085\n",
            "Trainable params: 104,085\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 11.2024 - accuracy: 0.4563 - val_loss: 0.8121 - val_accuracy: 0.6900\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.5808 - accuracy: 0.7887 - val_loss: 0.4966 - val_accuracy: 0.8200\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.3456 - accuracy: 0.8825 - val_loss: 0.4936 - val_accuracy: 0.8767\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 1s 35ms/step - loss: 0.2595 - accuracy: 0.9154 - val_loss: 0.4022 - val_accuracy: 0.8983\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 1s 35ms/step - loss: 0.1824 - accuracy: 0.9408 - val_loss: 0.4406 - val_accuracy: 0.8850\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.1495 - accuracy: 0.9508 - val_loss: 0.3502 - val_accuracy: 0.9150\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.1046 - accuracy: 0.9638 - val_loss: 0.4570 - val_accuracy: 0.9050\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 1s 35ms/step - loss: 0.0817 - accuracy: 0.9683 - val_loss: 0.4051 - val_accuracy: 0.9233\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0977 - accuracy: 0.9704 - val_loss: 0.3340 - val_accuracy: 0.9150\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 1s 35ms/step - loss: 0.0762 - accuracy: 0.9750 - val_loss: 0.4478 - val_accuracy: 0.9150\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0659 - accuracy: 0.9750 - val_loss: 0.3738 - val_accuracy: 0.9317\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0469 - accuracy: 0.9858 - val_loss: 0.3932 - val_accuracy: 0.9300\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0393 - accuracy: 0.9854 - val_loss: 0.3990 - val_accuracy: 0.9167\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0460 - accuracy: 0.9858 - val_loss: 0.5284 - val_accuracy: 0.9217\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0391 - accuracy: 0.9871 - val_loss: 0.4595 - val_accuracy: 0.9267\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 1s 35ms/step - loss: 0.0367 - accuracy: 0.9900 - val_loss: 0.5205 - val_accuracy: 0.9300\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0271 - accuracy: 0.9908 - val_loss: 0.8643 - val_accuracy: 0.9100\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.0625 - accuracy: 0.9821 - val_loss: 0.6388 - val_accuracy: 0.9250\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0703 - accuracy: 0.9821 - val_loss: 0.3905 - val_accuracy: 0.9183\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0368 - accuracy: 0.9871 - val_loss: 0.6257 - val_accuracy: 0.9200\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0174 - accuracy: 0.9962 - val_loss: 0.6169 - val_accuracy: 0.9183\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0227 - accuracy: 0.9933 - val_loss: 0.6324 - val_accuracy: 0.9283\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0312 - accuracy: 0.9912 - val_loss: 0.3926 - val_accuracy: 0.9317\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.0273 - accuracy: 0.9925 - val_loss: 0.4984 - val_accuracy: 0.9300\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0217 - accuracy: 0.9929 - val_loss: 0.6592 - val_accuracy: 0.9317\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0194 - accuracy: 0.9946 - val_loss: 0.8731 - val_accuracy: 0.9183\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0457 - accuracy: 0.9887 - val_loss: 0.5575 - val_accuracy: 0.9200\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0299 - accuracy: 0.9892 - val_loss: 0.6286 - val_accuracy: 0.9150\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0293 - accuracy: 0.9925 - val_loss: 0.5027 - val_accuracy: 0.9367\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0173 - accuracy: 0.9946 - val_loss: 0.6223 - val_accuracy: 0.9250\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0111 - accuracy: 0.9975 - val_loss: 0.8010 - val_accuracy: 0.9183\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0197 - accuracy: 0.9937 - val_loss: 0.7030 - val_accuracy: 0.9217\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 1s 39ms/step - loss: 0.0323 - accuracy: 0.9900 - val_loss: 0.6031 - val_accuracy: 0.9050\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0499 - accuracy: 0.9854 - val_loss: 0.5671 - val_accuracy: 0.9250\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0428 - accuracy: 0.9887 - val_loss: 0.6908 - val_accuracy: 0.9133\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0358 - accuracy: 0.9904 - val_loss: 0.6220 - val_accuracy: 0.9233\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0200 - accuracy: 0.9946 - val_loss: 0.5590 - val_accuracy: 0.9267\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.0149 - accuracy: 0.9933 - val_loss: 0.8480 - val_accuracy: 0.9283\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0109 - accuracy: 0.9962 - val_loss: 0.8533 - val_accuracy: 0.9200\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.0285 - accuracy: 0.9917 - val_loss: 0.6464 - val_accuracy: 0.9183\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0187 - accuracy: 0.9921 - val_loss: 0.6419 - val_accuracy: 0.9250\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0498 - accuracy: 0.9846 - val_loss: 0.8420 - val_accuracy: 0.9333\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 1s 39ms/step - loss: 0.0214 - accuracy: 0.9933 - val_loss: 0.8523 - val_accuracy: 0.9250\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0186 - accuracy: 0.9946 - val_loss: 0.7385 - val_accuracy: 0.9233\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0257 - accuracy: 0.9946 - val_loss: 0.6627 - val_accuracy: 0.9267\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.0137 - accuracy: 0.9962 - val_loss: 0.8247 - val_accuracy: 0.9200\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0173 - accuracy: 0.9946 - val_loss: 0.8200 - val_accuracy: 0.9233\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 1s 39ms/step - loss: 0.0131 - accuracy: 0.9958 - val_loss: 0.6908 - val_accuracy: 0.9367\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.0142 - accuracy: 0.9958 - val_loss: 0.8182 - val_accuracy: 0.9200\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0258 - accuracy: 0.9921 - val_loss: 0.9361 - val_accuracy: 0.9100\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.0336 - accuracy: 0.9917 - val_loss: 0.8965 - val_accuracy: 0.9117\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 1s 39ms/step - loss: 0.0438 - accuracy: 0.9892 - val_loss: 0.4970 - val_accuracy: 0.9250\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0235 - accuracy: 0.9929 - val_loss: 0.5118 - val_accuracy: 0.9333\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.0234 - accuracy: 0.9958 - val_loss: 0.6091 - val_accuracy: 0.9300\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.0106 - accuracy: 0.9962 - val_loss: 0.6741 - val_accuracy: 0.9250\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.0176 - accuracy: 0.9950 - val_loss: 0.8336 - val_accuracy: 0.9200\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.0380 - accuracy: 0.9908 - val_loss: 0.7068 - val_accuracy: 0.9333\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.0184 - accuracy: 0.9946 - val_loss: 1.0279 - val_accuracy: 0.9167\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0157 - accuracy: 0.9962 - val_loss: 0.6296 - val_accuracy: 0.9250\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0346 - accuracy: 0.9929 - val_loss: 0.6420 - val_accuracy: 0.9350\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0530 - accuracy: 0.9921 - val_loss: 0.8164 - val_accuracy: 0.9200\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0210 - accuracy: 0.9946 - val_loss: 0.5772 - val_accuracy: 0.9317\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0181 - accuracy: 0.9921 - val_loss: 0.7128 - val_accuracy: 0.9233\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.6875 - val_accuracy: 0.9267\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0058 - accuracy: 0.9975 - val_loss: 0.6864 - val_accuracy: 0.9250\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0044 - accuracy: 0.9983 - val_loss: 0.8553 - val_accuracy: 0.9283\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 8.1247e-04 - accuracy: 1.0000 - val_loss: 0.8337 - val_accuracy: 0.9283\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 3.2839e-04 - accuracy: 1.0000 - val_loss: 0.8079 - val_accuracy: 0.9267\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 5.5398e-04 - accuracy: 0.9996 - val_loss: 0.9590 - val_accuracy: 0.9333\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 1.0700 - val_accuracy: 0.9233\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 1.0342 - val_accuracy: 0.9283\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.0203 - accuracy: 0.9971 - val_loss: 1.0410 - val_accuracy: 0.9033\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0534 - accuracy: 0.9917 - val_loss: 0.7723 - val_accuracy: 0.9350\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.0579 - accuracy: 0.9896 - val_loss: 0.7918 - val_accuracy: 0.9283\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.0241 - accuracy: 0.9950 - val_loss: 0.7249 - val_accuracy: 0.9250\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0163 - accuracy: 0.9967 - val_loss: 0.8074 - val_accuracy: 0.9350\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0163 - accuracy: 0.9950 - val_loss: 0.6432 - val_accuracy: 0.9233\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0221 - accuracy: 0.9962 - val_loss: 0.7378 - val_accuracy: 0.9333\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0048 - accuracy: 0.9983 - val_loss: 0.7990 - val_accuracy: 0.9250\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 1s 39ms/step - loss: 0.0128 - accuracy: 0.9962 - val_loss: 0.6577 - val_accuracy: 0.9300\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0131 - accuracy: 0.9967 - val_loss: 0.8849 - val_accuracy: 0.9333\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0234 - accuracy: 0.9942 - val_loss: 0.8124 - val_accuracy: 0.9283\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0245 - accuracy: 0.9958 - val_loss: 0.6295 - val_accuracy: 0.9283\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 1s 39ms/step - loss: 0.0300 - accuracy: 0.9958 - val_loss: 0.7357 - val_accuracy: 0.9350\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0151 - accuracy: 0.9958 - val_loss: 0.6185 - val_accuracy: 0.9333\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6502 - val_accuracy: 0.9350\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0018 - accuracy: 0.9987 - val_loss: 0.8737 - val_accuracy: 0.9300\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0100 - accuracy: 0.9975 - val_loss: 0.8558 - val_accuracy: 0.9267\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0094 - accuracy: 0.9967 - val_loss: 0.8467 - val_accuracy: 0.9300\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0076 - accuracy: 0.9971 - val_loss: 0.8564 - val_accuracy: 0.9350\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0294 - accuracy: 0.9967 - val_loss: 0.7347 - val_accuracy: 0.9283\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0167 - accuracy: 0.9942 - val_loss: 0.7164 - val_accuracy: 0.9283\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0158 - accuracy: 0.9950 - val_loss: 0.8121 - val_accuracy: 0.9317\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0271 - accuracy: 0.9942 - val_loss: 0.5952 - val_accuracy: 0.9300\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0195 - accuracy: 0.9958 - val_loss: 0.6087 - val_accuracy: 0.9300\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0093 - accuracy: 0.9979 - val_loss: 0.8169 - val_accuracy: 0.9367\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0179 - accuracy: 0.9967 - val_loss: 0.9454 - val_accuracy: 0.9333\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.0068 - accuracy: 0.9975 - val_loss: 0.9500 - val_accuracy: 0.9367\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 0.0114 - accuracy: 0.9971 - val_loss: 0.9744 - val_accuracy: 0.9317\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.0059 - accuracy: 0.9975 - val_loss: 0.8976 - val_accuracy: 0.9267\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.8976 - accuracy: 0.9267\n",
            "[*] Writing output text file...\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}